{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import math\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "import gc\n",
    "\n",
    "import transformers\n",
    "from transformers.models.speech_to_text import Speech2TextConfig\n",
    "from transformers.models.speech_to_text.modeling_speech_to_text import shift_tokens_right, Speech2TextDecoder\n",
    "from Squeezeformer import SqueezeformerEncoder\n",
    "from timm.layers.norm_act import BatchNormAct2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# constants\n",
    "FEATURES = {\n",
    "    'hand': {\n",
    "        'left': range(21),\n",
    "        'right': range(21)\n",
    "    },\n",
    "    'pose': {\n",
    "        'left': [13, 15, 17, 19, 21],\n",
    "        'right': [14, 16, 18, 20, 22]\n",
    "    },\n",
    "    #'head': range(468)\n",
    "}\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(DEVICE)\n",
    "VAL_SPLIT = 0.8\n",
    "TEST_SPLIT = 0.2\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "FRAME_LEN = 128\n",
    "MAX_LEN = 384 # max number of frames\n",
    "MAX_PHRASE = 31 + 3 # max len from data + start, pad, end tokens\n",
    "\n",
    "START_TOKEN = 'S'\n",
    "PAD_TOKEN = 'P'\n",
    "END_TOKEN = 'E'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_columns(features: dict = FEATURES) -> list:\n",
    "    HAND_COLS = []\n",
    "    if 'hand' in features:\n",
    "        HAND_COLS = [\n",
    "            f'{d}_{o}_hand_{i}' \n",
    "            for o in ['right', 'left'] \n",
    "            for i in features['hand'][o] \n",
    "            for d in ['x', 'y', 'z']\n",
    "        ]\n",
    "        \n",
    "    POSE_COLS = []\n",
    "    if 'pose' in features:\n",
    "        POSE_COLS = [\n",
    "            f'{d}_pose_{i}' \n",
    "            for o in ['right', 'left'] \n",
    "            for i in features['pose'][o]\n",
    "            for d in ['x', 'y', 'z']\n",
    "        ]\n",
    "        \n",
    "    HEAD_COLS = []\n",
    "    if 'head' in features:\n",
    "        HEAD_COLS = [\n",
    "            f'{d}_head_{i}' \n",
    "            for i in features['head'] \n",
    "            for d in ['x', 'y', 'z']\n",
    "        ]\n",
    "    \n",
    "    return HAND_COLS + POSE_COLS + HEAD_COLS\n",
    "\n",
    "\n",
    "class FingerspellingDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 features: dict = FEATURES,\n",
    "                 train: str = True,\n",
    "                 transform = None\n",
    "                ):\n",
    "        \n",
    "        if train:\n",
    "            # dataset_path = '/kaggle/input/asl-fingerspelling/train_landmarks'\n",
    "            # dataset_file = '/kaggle/input/asl-fingerspelling/train.csv'\n",
    "            dataset_path = 'C:/MASTERS/SYDE Project/asl-fingerspelling/train_landmarks'\n",
    "            dataset_file = 'C:/MASTERS/SYDE Project/asl-fingerspelling/train.csv'\n",
    "        else:\n",
    "            # dataset_path = '/kaggle/input/asl-fingerspelling/supplemental_landmarks'\n",
    "            # dataset_file = '/kaggle/input/asl-fingerspelling/supplemental_metadata.csv'\n",
    "            dataset_path = 'C:/MASTERS/SYDE Project/asl-fingerspelling/supplemental_landmarks'\n",
    "            dataset_file = 'C:/MASTERS/SYDE Project/asl-fingerspelling/train.csv'\n",
    "        \n",
    "        self.dataset_path = dataset_path\n",
    "        self.dataset_df = pd.read_csv(dataset_file)\n",
    "        self.feature_columns = extract_columns(features)\n",
    "        self.transform = transform\n",
    "        \n",
    "        # fetch the data from the .parquet file\n",
    "        # filter out the non used columns\n",
    "        self.parquet_df = {\n",
    "            file.split('.')[0]: pq.read_table(\n",
    "                f\"{self.dataset_path}/{file.split('.')[0]}.parquet\",\n",
    "                columns=['sequence_id'] + self.feature_columns\n",
    "            ).to_pandas()\n",
    "            for file in os.listdir(dataset_path)\n",
    "        }\n",
    "        # convert parquet data to numpy\n",
    "        self.parquet_np = {\n",
    "            file_id: self.parquet_df[file_id].to_numpy() for file_id in self.parquet_df\n",
    "        }\n",
    "        \n",
    "        self.X_IDX = [i for i, col in enumerate(self.feature_columns)  if \"x_\" in col]\n",
    "        self.Y_IDX = [i for i, col in enumerate(self.feature_columns)  if \"y_\" in col]\n",
    "        self.Z_IDX = [i for i, col in enumerate(self.feature_columns)  if \"z_\" in col]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset_df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # convert to list of indices (if index is a tensor)\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "    \n",
    "        # locate sample in dataset dataframe\n",
    "        sequence_id, file_id, phrase = self.dataset_df.iloc[index][['sequence_id', 'file_id', 'phrase']]\n",
    "        \n",
    "        # filter dataset and fetch entries for the relevant file_id\n",
    "        file_df = self.dataset_df.loc[self.dataset_df[\"file_id\"] == file_id]\n",
    "    \n",
    "        # filter the parquet data by the sequence_id of the sample\n",
    "        frames = self.parquet_np[str(file_id)][self.parquet_df[str(file_id)].index == sequence_id]\n",
    "        indices_lists = [self.X_IDX, self.Y_IDX, self.Z_IDX]\n",
    "        frames = np.stack([frames[:, indices] for indices in indices_lists], axis=-1)\n",
    "        frames = frames.reshape(frames.shape[0], -1, len(indices_lists))\n",
    "\n",
    "        sample = {\n",
    "            'data': frames, # numpy.ndarray\n",
    "            'phrase': phrase, # string\n",
    "        }\n",
    "        \n",
    "        # apply transformation(s)\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "    \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizeAndFillNaNs(object):\n",
    "    def __init__(self):\n",
    "        super(NormalizeAndFillNaNs, self).__init__()\n",
    "    \n",
    "    def normalize(self,x):\n",
    "        nonan = x[~torch.isnan(x)].view(-1, x.shape[-1])\n",
    "        x = x - nonan.mean(0)[None, None, :]\n",
    "        x = x / nonan.std(0, unbiased=False)[None, None, :]\n",
    "        return x\n",
    "    \n",
    "    def fill_nans(self,x):\n",
    "        x[torch.isnan(x)] = 0\n",
    "        return x\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        x, phrase = sample['data'], sample['phrase']\n",
    "        #seq_len, 3* n_landmarks -> seq_len, n_landmarks, 3\n",
    "        x = x.reshape(x.shape[0],3,-1).permute(0,2,1)\n",
    "        \n",
    "        # Normalize & fill nans\n",
    "        x = self.normalize(x)\n",
    "        x = self.fill_nans(x)\n",
    "        \n",
    "        return {\n",
    "            'data': x, \n",
    "            'phrase': phrase\n",
    "        }\n",
    "\n",
    "class TokenizePhrase(object):\n",
    "    def __init__(self):\n",
    "        # with open('/kaggle/input/asl-fingerspelling/character_to_prediction_index.json', \"r\") as f:\n",
    "        with open('C:/MASTERS/SYDE Project/asl-fingerspelling/character_to_prediction_index.json', \"r\") as f:\n",
    "            self.char_to_num = json.load(f)\n",
    "        n = len(self.char_to_num)\n",
    "        self.char_to_num[PAD_TOKEN] = n\n",
    "        self.char_to_num[START_TOKEN] = n + 1\n",
    "        self.char_to_num[END_TOKEN] = n + 2\n",
    "        \n",
    "        self.num_to_char = {j:i for i,j in self.char_to_num.items()}\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        left_hand, right_hand, left_pose, right_pose, data_mask, phrase = sample['left_hand'], sample['right_hand'], sample['left_pose'], sample['right_pose'], sample['data_mask'], sample['phrase']\n",
    "        \n",
    "        start_token_id = self.char_to_num[START_TOKEN]\n",
    "        end_token_id = self.char_to_num[END_TOKEN]\n",
    "        pad_token_id = self.char_to_num[PAD_TOKEN]\n",
    "        phrase_tokens = [self.char_to_num[char] for char in phrase]\n",
    "        if len(phrase_tokens) > MAX_PHRASE - 1:\n",
    "            phrase_tokens = phrase_tokens[:MAX_PHRASE - 1]\n",
    "        phrase_tokens = [start_token_id] + phrase_tokens + [end_token_id]\n",
    "        phrase_mask = [1] * len(phrase_tokens)\n",
    "        \n",
    "        to_pad = MAX_PHRASE - len(phrase_tokens)\n",
    "        phrase_tokens = torch.tensor(phrase_tokens + [pad_token_id] * to_pad)\n",
    "        phrase_mask = torch.tensor(phrase_mask + [0] * to_pad)\n",
    "        \n",
    "        return {\n",
    "            'left_hand': left_hand, # tensor\n",
    "            'right_hand': right_hand, # tensor\n",
    "            'left_pose': left_pose, # tensor\n",
    "            'right_pose': right_pose, # tensor\n",
    "            'data_mask': data_mask, # tensor\n",
    "            \n",
    "            'phrase': phrase, # string\n",
    "            'phrase_tokens': phrase_tokens, # tensor (long)\n",
    "            'phrase_mask': phrase_mask # tensor (long)\n",
    "        }\n",
    "    \n",
    "class InterpolateOrPad(object):\n",
    "    def __init__(self, max_length: int = MAX_LEN):\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        data, phrase = sample['data'], sample['phrase']\n",
    "        diff = self.max_length - data.shape[0]\n",
    "        \n",
    "        # # crop\n",
    "        # if diff <= 0:\n",
    "        #     data = F.interpolate(data.permute(1,2,0),max_len).permute(2,0,1)\n",
    "        #     data_mask = torch.ones_like(data[:,0,0])\n",
    "        #     return data, mask\n",
    "\n",
    "        # pad\n",
    "        coef = 0\n",
    "        padding = torch.ones((diff, data.shape[1], data.shape[2]))\n",
    "        data_mask = torch.ones_like(data[:,0,0])\n",
    "        data = torch.cat([data, padding * coef])\n",
    "        data_mask = torch.cat([data_mask, padding[:,0,0] * coef])\n",
    "\n",
    "        return {\n",
    "            'data': data,\n",
    "            'data_mask': data_mask,\n",
    "            'phrase': phrase\n",
    "        }\n",
    "    \n",
    "class SplitData(object):\n",
    "    def __init__(self, features: dict = FEATURES):\n",
    "        columns = extract_columns(features)\n",
    "        \n",
    "        self.X_IDX = [i for i, col in enumerate(columns)  if \"x_\" in col]\n",
    "        self.Y_IDX = [i for i, col in enumerate(columns)  if \"y_\" in col]\n",
    "        self.Z_IDX = [i for i, col in enumerate(columns)  if \"z_\" in col]\n",
    "        \n",
    "        self.RHAND_IDX = list(set([int(i/3) for i, col in enumerate(columns)  if \"right\" in col]))\n",
    "        self.LHAND_IDX = list(set([int(i/3) for i, col in enumerate(columns)  if  \"left\" in col]))\n",
    "        self.RPOSE_IDX = list(set([int(i/3) for i, col in enumerate(columns)  if  \"pose\" in col and int(col[-2:]) in FEATURES['pose']['right']]))\n",
    "        self.LPOSE_IDX = list(set([int(i/3) for i, col in enumerate(columns)  if  \"pose\" in col and int(col[-2:]) in FEATURES['pose']['left']]))\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        data, data_mask, phrase = sample['data'], sample['data_mask'], sample['phrase']\n",
    "        data, phrase = sample['data'], sample['phrase']\n",
    "        return {\n",
    "            'left_hand': data[:, self.LHAND_IDX],\n",
    "            'right_hand': data[:, self.RHAND_IDX],\n",
    "            'left_pose': data[:, self.LPOSE_IDX],\n",
    "            'right_pose': data[:, self.RPOSE_IDX],\n",
    "            'data_mask': data_mask,\n",
    "            \n",
    "            'phrase': phrase,\n",
    "        }\n",
    "\n",
    "class ToTensor(object):\n",
    "    def __call__(self, sample):\n",
    "        frames, phrase = sample['data'], sample['phrase']\n",
    "        return {\n",
    "            'data': torch.from_numpy(frames), \n",
    "            'phrase': phrase\n",
    "        }\n",
    "    \n",
    "class Resample(object):\n",
    "    def __init__(self, rate=(0.8,1.2)):\n",
    "        self.rate = rate\n",
    "    \n",
    "    def interp1d_(self, x, new_size):\n",
    "        indices = torch.linspace(0, len(x) - 1, steps=new_size, dtype=torch.float32)\n",
    "        indices_floor = torch.floor(indices).to(torch.int64)\n",
    "        indices_frac = indices - indices_floor\n",
    "        indices_floor = torch.clamp(indices_floor, 0, len(x) - 2)\n",
    " \n",
    "        x0 = x[indices_floor]\n",
    "        x1 = x[indices_floor + 1]\n",
    "        indices_frac = indices_frac.view(-1, 1, 1)\n",
    "        new_x = x0 + (x1 - x0) * indices_frac\n",
    "        return new_x\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        frames, phrase = sample['data'], sample['phrase']\n",
    "        if torch.rand(1)>0.8:\n",
    "            rate = torch.FloatTensor(1).uniform_(self.rate[0], self.rate[1])\n",
    "            length = frames.shape[0]\n",
    "            new_size = int(rate * length)\n",
    "            new_frames = self.interp1d_(frames, new_size)\n",
    "        else:\n",
    "            new_frames = frames\n",
    "        return {'data': new_frames, 'phrase': phrase}\n",
    "    \n",
    "class SpatialRandomAffine(object):\n",
    "    def __init__(self, \n",
    "                 scale=(0.8, 1.2),\n",
    "                 shear=(-0.15, 0.15),\n",
    "                 shift=(-0.1, 0.1),\n",
    "                 degree=(-30, 30)\n",
    "                ):\n",
    "        self.scale = scale\n",
    "        self.shear = shear\n",
    "        self.shift = shift\n",
    "        self.degree = degree\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        data, phrase = sample['data'], sample['phrase']\n",
    "        if torch.rand(1)>0.75:\n",
    "            center = torch.tensor([0.5, 0.5])\n",
    "    \n",
    "            if self.scale is not None:\n",
    "                scale = torch.rand(1).item() * (self.scale[1] - self.scale[0]) + self.scale[0]\n",
    "                data = scale * data\n",
    "\n",
    "            if self.shear is not None:\n",
    "                xy = data[..., :2]\n",
    "                z = data[..., 2:]\n",
    "                shear_x = shear_y = torch.rand(1).item() * (self.shear[1] - self.shear[0]) + self.shear[0]\n",
    "                if torch.rand(1).item() < 0.5:\n",
    "                    shear_x = 0.0\n",
    "                else:\n",
    "                    shear_y = 0.0\n",
    "                shear_mat = torch.tensor([\n",
    "                    [1.0, shear_x],\n",
    "                    [shear_y, 1.0]\n",
    "                ])\n",
    "                xy = torch.matmul(xy, shear_mat)\n",
    "                center = center + torch.tensor([shear_y, shear_x])\n",
    "                data = torch.cat([xy, z], dim=-1)\n",
    "            \n",
    "            if self.degree is not None:\n",
    "                xy = data[..., :2]\n",
    "                z = data[..., 2:]\n",
    "                xy -= center\n",
    "                degree = torch.rand(1).item() * (self.degree[1] - self.degree[0]) + self.degree[0]\n",
    "                radian = degree / 180 * torch.tensor([3.14159265358979323846])\n",
    "                c = torch.cos(radian)\n",
    "                s = torch.sin(radian)\n",
    "                rotate_mat = torch.tensor([\n",
    "                    [c, s],\n",
    "                    [-s, c]\n",
    "                ])\n",
    "                xy = torch.matmul(xy, rotate_mat)\n",
    "                xy = xy + center\n",
    "                data = torch.cat([xy, z], dim=-1)\n",
    "\n",
    "            if self.shift is not None:\n",
    "                shift = torch.rand(1).item() * (self.shift[1] - self.shift[0]) + self.shift[0]\n",
    "                data = data + shift\n",
    "            \n",
    "        return {'data': data, 'phrase': phrase}\n",
    "    \n",
    "    \n",
    "class TemporalMask(object):\n",
    "    def __init__(self, size=(0.2,0.4), mask_value=float('nan')):\n",
    "        self.size = size\n",
    "        self.mask_value = mask_value\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        data, phrase = sample['data'], sample['phrase']\n",
    "        if torch.rand(1)>0.5:\n",
    "            l = data.shape[0]\n",
    "            mask_size = torch.rand(1).item() * (self.size[1] - self.size[0]) + self.size[0]\n",
    "            mask_size = int(l * mask_size)\n",
    "            mask_offset = torch.randint(0, l - mask_size + 1, (1,)).item()\n",
    "            mask_indices = torch.arange(mask_offset, mask_offset + mask_size).unsqueeze(1)\n",
    "            mask = torch.full((mask_size, 52, 3), self.mask_value, dtype=data.dtype)\n",
    "            data[mask_indices,...] = mask.unsqueeze(1)\n",
    "        \n",
    "        return {'data': data, 'phrase': phrase}\n",
    "    \n",
    "class SpatialMask(object):\n",
    "    def __init__(self, size=(0.2,0.4), mask_value=float('nan')):\n",
    "        self.size = size\n",
    "        self.mask_value = mask_value\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        # TODO: determine if this works as intended (does x/y refer to xyz coordinates?)\n",
    "        xyz, phrase = sample['data'], sample['phrase']\n",
    "        if torch.rand(1)>0.5:\n",
    "            mask_offset_y = torch.rand(1).item()\n",
    "            mask_offset_x = torch.rand(1).item()\n",
    "            mask_size = torch.rand(1).item() * (self.size[1] - self.size[0]) + self.size[0]\n",
    "\n",
    "            mask_x = (mask_offset_x < xyz[..., 0]) & (xyz[..., 0] < mask_offset_x + mask_size)\n",
    "            mask_y = (mask_offset_y < xyz[..., 1]) & (xyz[..., 1] < mask_offset_y + mask_size)\n",
    "            mask = mask_x & mask_y\n",
    "\n",
    "            xyz = torch.where(mask.unsqueeze(-1), torch.tensor(self.mask_value), xyz)\n",
    "\n",
    "        \n",
    "        return {'data': xyz, 'phrase': phrase}\n",
    "    \n",
    "# TODO: determine indices for left/right \n",
    "LEFT = []\n",
    "RIGHT = []\n",
    "\n",
    "class FlipLeftRight(object):\n",
    "    def __init__(self, left=LEFT, right=RIGHT):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        \n",
    "    # TODO: fix, not sure if non 3d data format will work (ie requires xyz to extend into 3rd dimension)\n",
    "    def __call__(self, sample):\n",
    "        xyz, phrase = sample['data'], sample['phrase']\n",
    "        if torch.rand(1)>0.5:\n",
    "            x, y, z = torch.unbind(xyz, dim=-1)\n",
    "            x = 1 - x\n",
    "            new_xyz = torch.stack([x, y, z], dim=-1)\n",
    "            new_xyz = new_xyz.transpose(0, 1)\n",
    "\n",
    "            l_x = new_xyz[self.left]\n",
    "            r_x = new_xyz[self.right]\n",
    "\n",
    "            for i in range(len(self.left)):\n",
    "                new_xyz[self.left[i]] = r_x[i]\n",
    "                new_xyz[self.right[i]] = l_x[i]\n",
    "\n",
    "            new_xyz = new_xyz.transpose(0, 1)\n",
    "        \n",
    "        else:\n",
    "            new_xyz = xyz\n",
    "\n",
    "        return {'data': new_xyz, 'phrase': phrase}\n",
    "\n",
    "class TemporalCrop(object):\n",
    "    def __init__(self, length=32):\n",
    "        self.length = length\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        data, phrase = sample['data'], sample['phrase']\n",
    "        \n",
    "        l = data.shape[0]\n",
    "        # TODO remove self.length<l-1 once fixed\n",
    "        if self.length is not None and self.length<l-1:\n",
    "            offset = torch.randint(0, l - self.length + 1, (1,)).item()\n",
    "            data = data[offset:offset + self.length]\n",
    "            \n",
    "        return {\n",
    "            'data': data, \n",
    "            'phrase': phrase\n",
    "        }\n",
    "    \n",
    "def get_data_loaders(features: dict = FEATURES, val_split: float = 0.8, test_split: float = 0.2, batch_size: int = 16):\n",
    "    # load datasets\n",
    "    transform = transforms.Compose([\n",
    "                ToTensor(),\n",
    "                NormalizeAndFillNaNs(),\n",
    "                Resample(),\n",
    "                SpatialRandomAffine(),\n",
    "                SpatialMask(),\n",
    "                TemporalMask(),\n",
    "                TemporalCrop(),\n",
    "                InterpolateOrPad(), \n",
    "                SplitData(),\n",
    "                TokenizePhrase() \n",
    "    ])\n",
    "    train_data = FingerspellingDataset(features=features, train=True, transform=transform) \n",
    "    \n",
    "    test_val_data = FingerspellingDataset(features=features, train=False, transform=transform) \n",
    "    dataset_size = len(test_val_data)\n",
    "    test_data, val_data = torch.utils.data.random_split(test_val_data, [math.ceil(test_split * dataset_size), math.floor(val_split * dataset_size)])\n",
    "\n",
    "    # setup data loaders\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=1000)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = FingerspellingDataset() # need to add transforms\n",
    "# train_loader,_,_ = get_data_loaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tr_it = iter(train_loader)\n",
    "\n",
    "# print(next(tr_it)['left_hand'].shape)\n",
    "# interpolateD = InterpolateOrPad()\n",
    "# print(train_data[1]['data'].shape)\n",
    "# print(interpolateD(train_data[1])['data'].shape)\n",
    "# print(interpolateD(train_data[1])['data_mask'].shape)\n",
    "# splitD = SplitData()\n",
    "# tokenD = TokenizePhrase()\n",
    "# print(list(tokenD(splitD(interpolateD(train_data[1]))).keys()))\n",
    "# print(tokenD(splitD(interpolateD(train_data[1])))['left_hand'].shape)\n",
    "# print(tokenD(splitD(interpolateD(train_data[1])))['phrase_tokens'])\n",
    "# print(tokenD(splitD(interpolateD(train_data[1])))['phrase_mask'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_landmarks,out_dim, conv_ch = 3):\n",
    "        super().__init__()   \n",
    "\n",
    "        self.in_channels = in_channels = 32 * math.ceil(n_landmarks / 2)\n",
    "        self.stem_linear = nn.Linear(in_channels,out_dim,bias=False)\n",
    "        self.stem_bn = nn.BatchNorm1d(out_dim, momentum=0.95)\n",
    "        self.conv_stem = nn.Conv2d(conv_ch, 32, kernel_size=(3, 3), stride=(1, 2), padding=(1, 1), bias=False)\n",
    "        self.bn_conv = BatchNormAct2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True,act_layer = nn.SiLU,drop_layer=None)\n",
    "        \n",
    "    def forward(self, data, mask):\n",
    "\n",
    "\n",
    "        xc = data.permute(0,3,1,2)\n",
    "        xc = self.conv_stem(xc)\n",
    "        xc = self.bn_conv(xc)\n",
    "        xc = xc.permute(0,2,3,1)\n",
    "        xc = xc.reshape(*data.shape[:2], -1)\n",
    "        \n",
    "        m = mask.to(torch.bool)  \n",
    "        x = self.stem_linear(xc)\n",
    "        \n",
    "        # Batchnorm without pads\n",
    "        bs,slen,nfeat = x.shape\n",
    "        x = x.view(-1, nfeat)\n",
    "        x_bn = x[mask.view(-1)==1].unsqueeze(0)\n",
    "        x_bn = self.stem_bn(x_bn.permute(0,2,1)).permute(0,2,1)\n",
    "        x[mask.view(-1)==1] = x_bn[0]\n",
    "        x = x.view(bs,slen,nfeat)\n",
    "        # Padding mask\n",
    "        x = x.masked_fill(~mask.bool().unsqueeze(-1), 0.0)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, decoder_config):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.config = decoder_config\n",
    "        self.decoder = Speech2TextDecoder(decoder_config) \n",
    "        self.lm_head = nn.Linear(decoder_config.d_model, decoder_config.vocab_size, bias=False)\n",
    "        \n",
    "        self.decoder_start_token_id = decoder_config.decoder_start_token_id\n",
    "        self.decoder_pad_token_id = decoder_config.pad_token_id #used for early stopping\n",
    "        self.decoder_end_token_id= decoder_config.eos_token_id\n",
    "        \n",
    "    def forward(self,x, labels=None, attention_mask = None, encoder_attention_mask = None):\n",
    "        \n",
    "        if labels is not None:\n",
    "            decoder_input_ids = shift_tokens_right(labels, self.config.pad_token_id, self.config.decoder_start_token_id)\n",
    "            \n",
    "        decoder_outputs = self.decoder(input_ids=decoder_input_ids,\n",
    "                                       encoder_hidden_states=x, \n",
    "                                       attention_mask = attention_mask,\n",
    "                                       encoder_attention_mask = encoder_attention_mask)\n",
    "        lm_logits = self.lm_head(decoder_outputs.last_hidden_state)\n",
    "        return lm_logits\n",
    "            \n",
    "    def generate(self, x, max_new_tokens=33, encoder_attention_mask=None):\n",
    "\n",
    "        decoder_input_ids = torch.ones((x.shape[0], 1), device=x.device, dtype=torch.long).fill_(self.decoder_start_token_id)\n",
    "        for i in range(max_new_tokens-1):  \n",
    "            decoder_outputs = self.decoder(input_ids=decoder_input_ids,encoder_hidden_states=x, encoder_attention_mask=encoder_attention_mask)\n",
    "            logits = self.lm_head(decoder_outputs.last_hidden_state)\n",
    "            decoder_input_ids = torch.cat([decoder_input_ids,logits.argmax(2)[:,-1:]],dim=1)\n",
    "\n",
    "            if torch.all((decoder_input_ids==self.decoder_end_token_id).sum(-1) > 0):\n",
    "                break\n",
    "                \n",
    "        return decoder_input_ids\n",
    "    \n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "\n",
    "        dim=208\n",
    "        n_handlandmarks = 21\n",
    "        n_poselandmarks = 5\n",
    "        n_landmarks = 2*n_handlandmarks+2*n_poselandmarks\n",
    "\n",
    "        d_cfg = Speech2TextConfig.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
    "        d_cfg.encoder_layers = 0\n",
    "        d_cfg.decoder_layers = 2\n",
    "        d_cfg.d_model = dim\n",
    "        d_cfg.max_target_positions = 1024 #?\n",
    "        d_cfg.num_hidden_layers = 1\n",
    "        d_cfg.vocab_size = 63\n",
    "        d_cfg.bos_token_id = 60\n",
    "        d_cfg.eos_token_id = 61\n",
    "        d_cfg.decoder_start_token_id = 60\n",
    "        d_cfg.pad_token_id = 59\n",
    "        d_cfg.num_conv_layers = 0\n",
    "        d_cfg.conv_kernel_sizes = []\n",
    "        d_cfg.max_length = dim\n",
    "        d_cfg.input_feat_per_channel = dim\n",
    "        d_cfg.num_beams = 1\n",
    "        d_cfg.attention_dropout = 0.2\n",
    "        d_cfg.decoder_ffn_dim = 512\n",
    "        d_cfg.init_std = 0.02\n",
    "        \n",
    "        self.feature_extractor = FeatureExtractor(n_landmarks=n_landmarks,out_dim=dim)\n",
    "        self.feature_extractor_lhand = FeatureExtractor(n_handlandmarks,out_dim=dim//4)\n",
    "        self.feature_extractor_rhand = FeatureExtractor(n_handlandmarks,out_dim=dim//4)\n",
    "        self.feature_extractor_lpose = FeatureExtractor(n_poselandmarks,out_dim=dim//4)\n",
    "        self.feature_extractor_rpose = FeatureExtractor(n_poselandmarks,out_dim=dim//4)\n",
    "        \n",
    "        self.encoder = SqueezeformerEncoder(\n",
    "                      input_dim=dim,\n",
    "                      encoder_dim=dim,\n",
    "                      num_layers=5,\n",
    "                      num_attention_heads= 4,\n",
    "                      feed_forward_expansion_factor=1,\n",
    "                      conv_expansion_factor= 2,\n",
    "                      input_dropout_p=0.1,\n",
    "                      feed_forward_dropout_p= 0.1,\n",
    "                      attention_dropout_p= 0.1,\n",
    "                      conv_dropout_p= 0.1,\n",
    "                      conv_kernel_size= 51,)\n",
    "        self.decoder = Decoder(d_cfg)\n",
    "        self.loss_fn = nn.CrossEntropyLoss() #done\n",
    "        print('n_params:',count_parameters(self))\n",
    "\n",
    "    def forward(self, batch):\n",
    "        #Concat,'rhand','lhand','lpose','rpose'\n",
    "        x = torch.cat([batch['left_hand'],batch['right_hand'],batch['left_pose'],batch['right_pose']],dim=-2)\n",
    "        labels = batch['phrase_tokens']\n",
    "        mask = batch['data_mask'].long()\n",
    "        label_mask = batch['phrase_mask']    \n",
    "\n",
    "        #maybe normalize\n",
    "        x_lhand = self.feature_extractor_lhand(batch['left_hand'].clone(), mask)\n",
    "        x_lpose = self.feature_extractor_lpose(batch['left_pose'].clone(), mask)\n",
    "        x_rhand = self.feature_extractor_rhand(batch['right_hand'].clone(), mask)\n",
    "        x_rpose = self.feature_extractor_rpose(batch['right_pose'].clone(), mask)\n",
    "        \n",
    "        x1 = torch.cat([x_lhand,x_rhand,x_lpose,x_rpose],dim=-1)\n",
    "        x = self.feature_extractor(x, mask)\n",
    "        x = x + x1\n",
    "        x = self.encoder(x, mask)\n",
    "        decoder_labels = labels.clone()        \n",
    "        \n",
    "        #??\n",
    "        # if self.training:\n",
    "        #     m = torch.rand(labels.shape) < self.decoder_mask_aug\n",
    "        #     decoder_labels[m] = 62\n",
    "\n",
    "        logits = self.decoder(x,\n",
    "                            labels=decoder_labels, \n",
    "                            encoder_attention_mask=mask.long()\n",
    "                            )\n",
    "        \n",
    "        loss = self.loss_fn(logits.view(-1, self.decoder.config.vocab_size), labels.view(-1))\n",
    "\n",
    "        output = {'loss':loss}\n",
    "\n",
    "        if not self.training:\n",
    "            generated_ids_padded = torch.ones((x.shape[0],self.max_phrase), dtype=torch.long, device=x.device) * 59\n",
    "            \n",
    "            if self.val_mode == 'padded':\n",
    "                generated_ids = self.decoder.generate(x,max_new_tokens=self.max_phrase + 1, encoder_attention_mask=mask.long())\n",
    "                    \n",
    "            elif self.val_mode == 'cutted':\n",
    "                generated_ids = torch.ones((x.shape[0],self.max_phrase+1), dtype=torch.long, device=x.device) * 59\n",
    "                mask_lens = mask.sum(1)\n",
    "                for lidx in mask_lens.unique():\n",
    "                    liddx = lidx == mask_lens\n",
    "                    preds = self.decoder.generate(x[liddx, :lidx],max_new_tokens=self.max_phrase + 1)\n",
    "                    generated_ids[liddx, :preds.shape[1]] = preds\n",
    "                    \n",
    "            cutoffs = (generated_ids==self.decoder.decoder_end_token_id).float().argmax(1).clamp(0,self.max_phrase)\n",
    "            for i, c in enumerate(cutoffs):\n",
    "                generated_ids_padded[i,:c] = generated_ids[i,:c]\n",
    "            output['generated_ids'] = generated_ids_padded\n",
    "            output['seq_len'] = batch['seq_len']    \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_dict = {'lhand': torch.rand(1,380,21,3),\n",
    "#             'rhand':torch.rand(1,380,21,3),\n",
    "#             'lpose':torch.rand(1,380,5,3),\n",
    "#             'rpose':torch.rand(1,380,5,3),\n",
    "#             'token_ids':(torch.rand(1,32)*52).long(),\n",
    "#             'input_mask': torch.ones_like(torch.rand(1,380)),\n",
    "#             'attention_mask': torch.ones_like(torch.rand(1,32))         \n",
    "# }\n",
    "# tr_it = iter(train_loader)\n",
    "# model = Net()\n",
    "\n",
    "# print(model(next(tr_it)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEED TO FIX AFTER THIS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, train_loader, optimizer,scheduler,epoch):\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    loss_history = []\n",
    "\n",
    "    model = model.to(DEVICE)\n",
    "    # set model to training mode\n",
    "    model.train()  \n",
    "    progress_bar = tqdm(range(len(train_loader))[:],desc=f'Epoch {epoch} Progress ')\n",
    "    tr_it = iter(train_loader)\n",
    "\n",
    "    for itr in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        data = next(tr_it)\n",
    "        batch = {key: data[key].to(DEVICE) for key in data if key != 'phrase'}\n",
    "        output = model(batch)\n",
    "        loss = output['loss']\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        scheduler.step()\n",
    "\n",
    "        # track some values to compute statistics\n",
    "        total_loss += loss.item()\n",
    "        # preds = torch.argmax(outputs, dim=-1)\n",
    "        # all_predictions.extend(preds.detach().cpu().tolist())\n",
    "        # all_targets.extend(targets.cpu().tolist())\n",
    "\n",
    "    # acc = accuracy_score(all_targets, all_predictions)\n",
    "    final_loss = total_loss / len(train_loader)\n",
    "    \n",
    "    # print average loss and accuracy\n",
    "    print(f\"learning Rate = {optimizer.param_groups[0]['lr']}. average train loss = {final_loss:.2f}\")\n",
    "    return final_loss\n",
    "\n",
    "def validation(model, val_loader, loss_fn,epoch):\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    model = model.to(DEVICE)\n",
    "    # set model to evaluation mode\n",
    "    model.eval()  \n",
    "    for i, (inputs, targets) in enumerate(val_loader):\n",
    "        with torch.no_grad():\n",
    "            model.to(DEVICE)\n",
    "            outputs = model(inputs.to(DEVICE))\n",
    "            loss = loss_fn(outputs, targets.to(DEVICE))\n",
    "\n",
    "            # Track some values to compute statistics\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=-1)\n",
    "            all_predictions.extend(preds.detach().cpu().tolist())\n",
    "            all_targets.extend(targets.cpu().tolist())\n",
    "\n",
    "    acc = accuracy_score(all_targets, all_predictions)\n",
    "    final_loss = total_loss / len(val_loader)\n",
    "    # Print average loss and accuracy\n",
    "    print(f\"Epoch {epoch + 1} done. average validation loss = {final_loss:.2f}, average validation accuracy = {acc * 100:.3f}%\")\n",
    "    return acc, final_loss\n",
    "\n",
    "def test(model, test_loader, loss_fn):\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    model = model.to(DEVICE)\n",
    "    # set model to evaluation mode\n",
    "    model.eval()\n",
    "    for i, (inputs, targets) in enumerate(test_loader):\n",
    "        with torch.no_grad():\n",
    "            model.to(DEVICE)\n",
    "            outputs = model(inputs.to(DEVICE))\n",
    "            loss = loss_fn(outputs, targets.to(DEVICE))\n",
    "\n",
    "            # Track some values to compute statistics\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=-1)\n",
    "            all_predictions.extend(preds.detach().cpu().tolist())\n",
    "            all_targets.extend(targets.cpu().tolist())\n",
    "\n",
    "    acc = accuracy_score(all_targets, all_predictions)\n",
    "    final_loss = total_loss / len(test_loader)\n",
    "    # Print average loss and accuracy\n",
    "    print(f'average test loss = {final_loss:.2f}, average test accuracy = {acc * 100:.3f}%')\n",
    "    return acc, final_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "                data_loaders: tuple = None,\n",
    "                learning_rate: float = 4.5e-3, \n",
    "                weight_decay: float = 0.08, \n",
    "                features: dict = None,\n",
    "                warmup_steps=1,\n",
    "                training_steps = 10\n",
    "               ):\n",
    "    \n",
    "    if not features:\n",
    "        features = FEATURES\n",
    "        \n",
    "    if not data_loaders:\n",
    "        train_loader, val_loader, test_loader = get_data_loaders(features, VAL_SPLIT, TEST_SPLIT, BATCH_SIZE)\n",
    "    else:\n",
    "        train_loader, val_loader, test_loader = data_loaders\n",
    "\n",
    "    model = Net().to(DEVICE)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = transformers.get_cosine_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=warmup_steps*len(train_loader),num_training_steps=training_steps*len(train_loader),num_cycles=0.5\n",
    "    )\n",
    "\n",
    "    max_epochs = 16\n",
    "    best_acc = -1\n",
    "    dip_count = 0\n",
    "    num_epochs = 0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for e in range(max_epochs):\n",
    "        gc.collect()\n",
    "        train_loss = train(model, train_loader, optimizer,scheduler,e)\n",
    "        \n",
    "         # early stopping based on train acc\n",
    "        # if e%2 == 0:\n",
    "        #     val_acc, val_loss = validation(model, val_loader)\n",
    "        #     if val_acc >= best_acc:\n",
    "        #         best_acc = val_acc\n",
    "        #         dip_count = 0\n",
    "        #     else:\n",
    "        #         dip_count +=1\n",
    "\n",
    "        # if dip_count >1:\n",
    "        #     break\n",
    "\n",
    "    # val_acc, val_loss = validation(model, val_loader)\n",
    "    # print(f'Final Accuracy: {val_acc}')\n",
    "    # return val_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_params: 4058336\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0af3996f55943148a3d6e9cb373140a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0 Progress :   0%|          | 0/1051 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 384, 52, 3])\n",
      "torch.Size([64, 384, 52, 3])\n",
      "torch.Size([64, 384, 52, 3])\n",
      "torch.Size([64, 384, 52, 3])\n",
      "torch.Size([64, 384, 52, 3])\n",
      "torch.Size([64, 384, 52, 3])\n",
      "torch.Size([64, 384, 52, 3])\n",
      "torch.Size([64, 384, 52, 3])\n",
      "torch.Size([64, 384, 52, 3])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\MASTERS\\SYDE Project\\model.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/MASTERS/SYDE%20Project/model.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train_model()\n",
      "\u001b[1;32mc:\\MASTERS\\SYDE Project\\model.ipynb Cell 15\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/MASTERS/SYDE%20Project/model.ipynb#X20sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_epochs):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/MASTERS/SYDE%20Project/model.ipynb#X20sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     gc\u001b[39m.\u001b[39mcollect()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/MASTERS/SYDE%20Project/model.ipynb#X20sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     train_loss \u001b[39m=\u001b[39m train(model, train_loader, optimizer,scheduler,e)\n",
      "\u001b[1;32mc:\\MASTERS\\SYDE Project\\model.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/MASTERS/SYDE%20Project/model.ipynb#X20sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(tr_it)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/MASTERS/SYDE%20Project/model.ipynb#X20sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m batch \u001b[39m=\u001b[39m {key: data[key]\u001b[39m.\u001b[39mto(DEVICE) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m data \u001b[39mif\u001b[39;00m key \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mphrase\u001b[39m\u001b[39m'\u001b[39m}\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/MASTERS/SYDE%20Project/model.ipynb#X20sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m output \u001b[39m=\u001b[39m model(batch)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/MASTERS/SYDE%20Project/model.ipynb#X20sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m loss \u001b[39m=\u001b[39m output[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/MASTERS/SYDE%20Project/model.ipynb#X20sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\kevin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kevin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32mc:\\MASTERS\\SYDE Project\\model.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/MASTERS/SYDE%20Project/model.ipynb#X20sZmlsZQ%3D%3D?line=143'>144</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_extractor(x, mask)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/MASTERS/SYDE%20Project/model.ipynb#X20sZmlsZQ%3D%3D?line=144'>145</a>\u001b[0m x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m x1\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/MASTERS/SYDE%20Project/model.ipynb#X20sZmlsZQ%3D%3D?line=145'>146</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(x, mask)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/MASTERS/SYDE%20Project/model.ipynb#X20sZmlsZQ%3D%3D?line=146'>147</a>\u001b[0m decoder_labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mclone()        \n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/MASTERS/SYDE%20Project/model.ipynb#X20sZmlsZQ%3D%3D?line=148'>149</a>\u001b[0m \u001b[39m#??\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/MASTERS/SYDE%20Project/model.ipynb#X20sZmlsZQ%3D%3D?line=149'>150</a>\u001b[0m \u001b[39m# if self.training:\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/MASTERS/SYDE%20Project/model.ipynb#X20sZmlsZQ%3D%3D?line=150'>151</a>\u001b[0m \u001b[39m#     m = torch.rand(labels.shape) < self.decoder_mask_aug\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/MASTERS/SYDE%20Project/model.ipynb#X20sZmlsZQ%3D%3D?line=151'>152</a>\u001b[0m \u001b[39m#     decoder_labels[m] = 62\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kevin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kevin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\MASTERS\\SYDE Project\\Squeezeformer.py:607\u001b[0m, in \u001b[0;36mSqueezeformerEncoder.forward\u001b[1;34m(self, x, mask)\u001b[0m\n\u001b[0;32m    593\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    594\u001b[0m \u001b[39mForward propagate a `inputs` for  encoder training.\u001b[39;00m\n\u001b[0;32m    595\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    603\u001b[0m \u001b[39m    * output_lengths (torch.LongTensor): The length of output tensor. ``(batch)``\u001b[39;00m\n\u001b[0;32m    604\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    606\u001b[0m \u001b[39mfor\u001b[39;00m idx, block \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks):\n\u001b[1;32m--> 607\u001b[0m     x \u001b[39m=\u001b[39m block(x, mask)\n\u001b[0;32m    609\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\kevin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kevin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\MASTERS\\SYDE Project\\Squeezeformer.py:693\u001b[0m, in \u001b[0;36mSqueezeformerBlock.forward\u001b[1;34m(self, x, mask)\u001b[0m\n\u001b[0;32m    691\u001b[0m \u001b[39m# Skip pad #1\u001b[39;00m\n\u001b[0;32m    692\u001b[0m x_skip \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, x\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m--> 693\u001b[0m x \u001b[39m=\u001b[39m x_skip[mask_flat]\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[0;32m    695\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_mhsa(x)\n\u001b[0;32m    697\u001b[0m residual \u001b[39m=\u001b[39m x\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
