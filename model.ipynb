{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchinfo in c:\\users\\kevin\\anaconda3\\lib\\site-packages (1.8.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import math\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from transformers.models.speech_to_text import Speech2TextConfig\n",
    "from transformers.models.speech_to_text.modeling_speech_to_text import shift_tokens_right, Speech2TextDecoder\n",
    "from Squeezeformer import SqueezeformerEncoder\n",
    "from timm.layers.norm_act import BatchNormAct2d\n",
    "from torchinfo import summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "FEATURES = {\n",
    "    'hand': {\n",
    "        'left': range(21),\n",
    "        'right': range(21)\n",
    "    },\n",
    "    'pose': {\n",
    "        'left': [13, 15, 17, 19, 21],\n",
    "        'right': [14, 16, 18, 20, 22]\n",
    "    },\n",
    "    #'head': range(468)\n",
    "}\n",
    "\n",
    "FRAME_LEN = 128\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "VAL_SPLIT = 0.8\n",
    "TEST_SPLIT = 0.2\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FingerspellingDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 features: dict = FEATURES,\n",
    "                 train: str = True,\n",
    "                 transform = None\n",
    "                ):\n",
    "        HAND_COLS = []\n",
    "        if 'hand' in features:\n",
    "            HAND_COLS = [\n",
    "                f'{d}_{o}_hand_{i}' \n",
    "                for o in ['right', 'left'] \n",
    "                for i in features['hand'][o] \n",
    "                for d in ['x', 'y', 'z']\n",
    "            ]\n",
    "        POSE_COLS = []\n",
    "        if 'pose' in features:\n",
    "            POSE_COLS = [\n",
    "                f'{d}_pose_{i}' \n",
    "                for o in ['right', 'left'] \n",
    "                for i in features['pose'][o]\n",
    "                for d in ['x', 'y', 'z']\n",
    "            ]\n",
    "        HEAD_COLS = []\n",
    "        if 'head' in features:\n",
    "            HEAD_COLS = [\n",
    "                f'{d}_head_{i}' \n",
    "                for i in features['head'] \n",
    "                for d in ['x', 'y', 'z']\n",
    "            ]\n",
    " \n",
    "        if train:\n",
    "            # dataset_path = '/kaggle/input/asl-fingerspelling/train_landmarks'\n",
    "            # dataset_file = '/kaggle/input/asl-fingerspelling/train.csv'\n",
    "            dataset_path = 'C:/Users/kevin/OneDrive/Desktop/MASTERS/SYDE Project/asl-fingerspelling/train_landmarks'\n",
    "            dataset_file = 'C:/Users/kevin/OneDrive/Desktop/MASTERS/SYDE Project/asl-fingerspelling/train.csv'\n",
    "        else:\n",
    "            # dataset_path = '/kaggle/input/asl-fingerspelling/supplemental_landmarks'\n",
    "            # dataset_file = '/kaggle/input/asl-fingerspelling/supplemental_metadata.csv'\n",
    "            dataset_path = 'C:/Users/kevin/OneDrive/Desktop/MASTERS/SYDE Project/asl-fingerspelling/supplemental_landmarks'\n",
    "            dataset_file = 'C:/Users/kevin/OneDrive/Desktop/MASTERS/SYDE Project/asl-fingerspelling/supplemental_metadata.csv'\n",
    "        self.dataset_path = dataset_path\n",
    "        self.dataset_df = pd.read_csv(dataset_file)\n",
    "        self.feature_columns = HAND_COLS + POSE_COLS + HEAD_COLS\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.dataset_df)\n",
    "    def __getitem__(self, index):\n",
    "        # convert to list of indices (if index is a tensor)\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "        # locate sample in dataset dataframe\n",
    "        sequence_id, file_id, phrase = self.dataset_df.iloc[index][['sequence_id', 'file_id', 'phrase']]\n",
    "        # filter dataset and fetch entries for the relevant file_id\n",
    "        file_df = self.dataset_df.loc[self.dataset_df[\"file_id\"] == file_id]\n",
    "        # fetch the data from the .parquet file\n",
    "        # filter out the non used columns\n",
    "        parquet_df = pq.read_table(\n",
    "                        f\"{self.dataset_path}/{str(file_id)}.parquet\",\n",
    "                        columns=['sequence_id'] + self.feature_columns\n",
    "                     ).to_pandas()\n",
    "        # convert parquet data to numpy\n",
    "        parquet_np = parquet_df.to_numpy()\n",
    "        # filter the parquet data by the sequence_id of the sample\n",
    "        frames = parquet_np[parquet_df.index == sequence_id]\n",
    " \n",
    "        sample = {\n",
    "            'data': frames, # numpy.ndarray\n",
    "            'phrase': phrase, # string\n",
    "        }\n",
    "        # apply transformation(s)\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    " \n",
    "    def __call__(self, sample):\n",
    "        frames, phrase = sample['data'], sample['phrase']\n",
    "        return {\n",
    "            'data': torch.from_numpy(frames), \n",
    "            'phrase': phrase\n",
    "        }\n",
    "\n",
    "def get_data_loaders(features: dict = FEATURES, val_split: float = 0.8, test_split: float = 0.2, batch_size: int = 16):\n",
    "    # load datasets\n",
    "    transform = transforms.Compose([ToTensor()])\n",
    "    train_data = FingerspellingDataset(features=features, train=True, transform=transform) # need to add transforms\n",
    "    test_val_data = FingerspellingDataset(features=features, train=False, transform=transform) # need to add transforms\n",
    "    dataset_size = len(test_val_data)\n",
    "    test_data, val_data = torch.utils.data.random_split(test_val_data, [math.ceil(test_split * dataset_size), math.floor(val_split * dataset_size)])\n",
    " \n",
    "    # setup data loaders\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=1000)\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[ 2.3356757e-01,  8.7151617e-01,  1.4675981e-06, ...,\n",
       "          1.2006173e+00,  1.3686731e+00, -1.2188929e+00],\n",
       "        [ 2.5071117e-01,  8.7850654e-01,  1.1639925e-06, ...,\n",
       "          1.1771251e+00,  1.4146559e+00, -1.3279152e+00],\n",
       "        [ 2.5689089e-01,  8.8893843e-01,  8.6225799e-07, ...,\n",
       "          1.1739606e+00,  1.4577438e+00, -1.3690206e+00],\n",
       "        ...,\n",
       "        [ 3.0872941e-01,  8.0841637e-01,  7.0709848e-07, ...,\n",
       "          1.1138570e+00,  1.4244299e+00, -8.6565697e-01],\n",
       "        [ 2.8616741e-01,  8.2798451e-01,  1.3166692e-07, ...,\n",
       "          1.1295004e+00,  1.4395330e+00, -8.9529443e-01],\n",
       "        [ 2.7528158e-01,  8.1241024e-01, -1.1747585e-07, ...,\n",
       "          1.1178130e+00,  1.4387276e+00, -7.9332572e-01]], dtype=float32),\n",
       " 'phrase': '988 franklin lane'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = FingerspellingDataset() # need to add transforms\n",
    "train_data[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_landmarks,out_dim, conv_ch = 3):\n",
    "        super().__init__()   \n",
    "\n",
    "        self.in_channels = in_channels = 32 * math.ceil(n_landmarks / 2)\n",
    "        self.stem_linear = nn.Linear(in_channels,out_dim,bias=False)\n",
    "        self.stem_bn = nn.BatchNorm1d(out_dim, momentum=0.95)\n",
    "        self.conv_stem = nn.Conv2d(conv_ch, 32, kernel_size=(3, 3), stride=(1, 2), padding=(1, 1), bias=False)\n",
    "        self.bn_conv = BatchNormAct2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True,act_layer = nn.SiLU,drop_layer=None)\n",
    "        \n",
    "    def forward(self, data, mask):\n",
    "\n",
    "\n",
    "        xc = data.permute(0,3,1,2)\n",
    "        xc = self.conv_stem(xc)\n",
    "        xc = self.bn_conv(xc)\n",
    "        xc = xc.permute(0,2,3,1)\n",
    "        xc = xc.reshape(*data.shape[:2], -1)\n",
    "        \n",
    "        m = mask.to(torch.bool)  \n",
    "        x = self.stem_linear(xc)\n",
    "        \n",
    "        # Batchnorm without pads\n",
    "        bs,slen,nfeat = x.shape\n",
    "        x = x.view(-1, nfeat)\n",
    "        x_bn = x[mask.view(-1)==1].unsqueeze(0)\n",
    "        x_bn = self.stem_bn(x_bn.permute(0,2,1)).permute(0,2,1)\n",
    "        x[mask.view(-1)==1] = x_bn[0]\n",
    "        x = x.view(bs,slen,nfeat)\n",
    "        # Padding mask\n",
    "        x = x.masked_fill(~mask.bool().unsqueeze(-1), 0.0)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, decoder_config):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.config = decoder_config\n",
    "        self.decoder = Speech2TextDecoder(decoder_config) \n",
    "        self.lm_head = nn.Linear(decoder_config.d_model, decoder_config.vocab_size, bias=False)\n",
    "        \n",
    "        self.decoder_start_token_id = decoder_config.decoder_start_token_id\n",
    "        self.decoder_pad_token_id = decoder_config.pad_token_id #used for early stopping\n",
    "        self.decoder_end_token_id= decoder_config.eos_token_id\n",
    "        \n",
    "    def forward(self,x, labels=None, attention_mask = None, encoder_attention_mask = None):\n",
    "        \n",
    "        if labels is not None:\n",
    "            decoder_input_ids = shift_tokens_right(labels, self.config.pad_token_id, self.config.decoder_start_token_id)\n",
    "            \n",
    "        decoder_outputs = self.decoder(input_ids=decoder_input_ids,\n",
    "                                       encoder_hidden_states=x, \n",
    "                                       attention_mask = attention_mask,\n",
    "                                       encoder_attention_mask = encoder_attention_mask)\n",
    "        lm_logits = self.lm_head(decoder_outputs.last_hidden_state)\n",
    "        return lm_logits\n",
    "            \n",
    "    def generate(self, x, max_new_tokens=33, encoder_attention_mask=None):\n",
    "\n",
    "        decoder_input_ids = torch.ones((x.shape[0], 1), device=x.device, dtype=torch.long).fill_(self.decoder_start_token_id)\n",
    "        for i in range(max_new_tokens-1):  \n",
    "            decoder_outputs = self.decoder(input_ids=decoder_input_ids,encoder_hidden_states=x, encoder_attention_mask=encoder_attention_mask)\n",
    "            logits = self.lm_head(decoder_outputs.last_hidden_state)\n",
    "            decoder_input_ids = torch.cat([decoder_input_ids,logits.argmax(2)[:,-1:]],dim=1)\n",
    "\n",
    "            if torch.all((decoder_input_ids==self.decoder_end_token_id).sum(-1) > 0):\n",
    "                break\n",
    "                \n",
    "        return decoder_input_ids\n",
    "    \n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "\n",
    "        dim=208\n",
    "        n_handlandmarks = 21\n",
    "        n_poselandmarks = 5\n",
    "        n_landmarks = 2*n_handlandmarks+2*n_poselandmarks\n",
    "\n",
    "        d_cfg = Speech2TextConfig.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
    "        d_cfg.encoder_layers = 0\n",
    "        d_cfg.decoder_layers = 2\n",
    "        d_cfg.d_model = dim\n",
    "        d_cfg.max_target_positions = 1024 #?\n",
    "        d_cfg.num_hidden_layers = 1\n",
    "        d_cfg.vocab_size = 63\n",
    "        d_cfg.bos_token_id = 59\n",
    "        d_cfg.eos_token_id = 60\n",
    "        d_cfg.decoder_start_token_id = 59\n",
    "        d_cfg.pad_token_id = 61\n",
    "        d_cfg.num_conv_layers = 0\n",
    "        d_cfg.conv_kernel_sizes = []\n",
    "        d_cfg.max_length = dim\n",
    "        d_cfg.input_feat_per_channel = dim\n",
    "        d_cfg.num_beams = 1\n",
    "        d_cfg.attention_dropout = 0.2\n",
    "        d_cfg.decoder_ffn_dim = 512\n",
    "        d_cfg.init_std = 0.02\n",
    "        \n",
    "        self.feature_extractor = FeatureExtractor(n_landmarks=n_landmarks,out_dim=dim)\n",
    "        self.feature_extractor_lhand = FeatureExtractor(n_handlandmarks,out_dim=dim//4)\n",
    "        self.feature_extractor_rhand = FeatureExtractor(n_handlandmarks,out_dim=dim//4)\n",
    "        self.feature_extractor_lpose = FeatureExtractor(n_poselandmarks,out_dim=dim//4)\n",
    "        self.feature_extractor_rpose = FeatureExtractor(n_poselandmarks,out_dim=dim//4)\n",
    "        \n",
    "        self.encoder = SqueezeformerEncoder(\n",
    "                      input_dim=dim,\n",
    "                      encoder_dim=dim,\n",
    "                      num_layers=5,\n",
    "                      num_attention_heads= 4,\n",
    "                      feed_forward_expansion_factor=1,\n",
    "                      conv_expansion_factor= 2,\n",
    "                      input_dropout_p=0.1,\n",
    "                      feed_forward_dropout_p= 0.1,\n",
    "                      attention_dropout_p= 0.1,\n",
    "                      conv_dropout_p= 0.1,\n",
    "                      conv_kernel_size= 51,)\n",
    "        self.decoder = Decoder(d_cfg)\n",
    "        self.loss_fn = nn.CrossEntropyLoss() #done\n",
    "        print('n_params:',count_parameters(self))\n",
    "\n",
    "    def forward(self, batch):\n",
    "        #Concat,'rhand','lhand','lpose','rpose'\n",
    "        x = torch.cat([batch['lhand'],batch['rhand'],batch['lpose'],batch['rpose']],dim=-2)\n",
    "        labels = batch['token_ids']\n",
    "        mask = batch['input_mask'].long()\n",
    "        label_mask = batch['attention_mask']    \n",
    "\n",
    "        #maybe normalize\n",
    "        x_lhand = self.feature_extractor_lhand(batch['lhand'].clone(), mask)\n",
    "        x_lpose = self.feature_extractor_lpose(batch['lpose'].clone(), mask)\n",
    "        x_rhand = self.feature_extractor_rhand(batch['rhand'].clone(), mask)\n",
    "        x_rpose = self.feature_extractor_rpose(batch['rpose'].clone(), mask)\n",
    "        \n",
    "        x1 = torch.cat([x_lhand,x_rhand,x_lpose,x_rpose],dim=-1)\n",
    "        x = self.feature_extractor(x, mask)\n",
    "        x = x + x1\n",
    "        x = self.encoder(x, mask)\n",
    "        decoder_labels = labels.clone()        \n",
    "        \n",
    "        #??\n",
    "        # if self.training:\n",
    "        #     m = torch.rand(labels.shape) < self.decoder_mask_aug\n",
    "        #     decoder_labels[m] = 62\n",
    "\n",
    "        logits = self.decoder(x,\n",
    "                            labels=decoder_labels, \n",
    "                            encoder_attention_mask=mask.long()\n",
    "                            )\n",
    "        \n",
    "        loss = self.loss_fn(logits.view(-1, self.decoder.config.vocab_size), labels.view(-1))\n",
    "\n",
    "        output = {'loss':loss}\n",
    "\n",
    "        if not self.training:\n",
    "            generated_ids_padded = torch.ones((x.shape[0],self.max_phrase), dtype=torch.long, device=x.device) * 59\n",
    "            \n",
    "            if self.val_mode == 'padded':\n",
    "                generated_ids = self.decoder.generate(x,max_new_tokens=self.max_phrase + 1, encoder_attention_mask=mask.long())\n",
    "                    \n",
    "            elif self.val_mode == 'cutted':\n",
    "                generated_ids = torch.ones((x.shape[0],self.max_phrase+1), dtype=torch.long, device=x.device) * 59\n",
    "                mask_lens = mask.sum(1)\n",
    "                for lidx in mask_lens.unique():\n",
    "                    liddx = lidx == mask_lens\n",
    "                    preds = self.decoder.generate(x[liddx, :lidx],max_new_tokens=self.max_phrase + 1)\n",
    "                    generated_ids[liddx, :preds.shape[1]] = preds\n",
    "                    \n",
    "            cutoffs = (generated_ids==self.decoder.decoder_end_token_id).float().argmax(1).clamp(0,self.max_phrase)\n",
    "            for i, c in enumerate(cutoffs):\n",
    "                generated_ids_padded[i,:c] = generated_ids[i,:c]\n",
    "            output['generated_ids'] = generated_ids_padded\n",
    "            output['seq_len'] = batch['seq_len']    \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_params: 4050016\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SqueezeformerEncoder' object has no attribute 'blocks'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\kevin\\OneDrive\\Desktop\\MASTERS\\SYDE Project\\model.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kevin/OneDrive/Desktop/MASTERS/SYDE%20Project/model.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m input_dict \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mlhand\u001b[39m\u001b[39m'\u001b[39m: torch\u001b[39m.\u001b[39mrand(\u001b[39m1\u001b[39m,\u001b[39m380\u001b[39m,\u001b[39m21\u001b[39m,\u001b[39m3\u001b[39m),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kevin/OneDrive/Desktop/MASTERS/SYDE%20Project/model.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mrhand\u001b[39m\u001b[39m'\u001b[39m:torch\u001b[39m.\u001b[39mrand(\u001b[39m1\u001b[39m,\u001b[39m380\u001b[39m,\u001b[39m21\u001b[39m,\u001b[39m3\u001b[39m),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kevin/OneDrive/Desktop/MASTERS/SYDE%20Project/model.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mlpose\u001b[39m\u001b[39m'\u001b[39m:torch\u001b[39m.\u001b[39mrand(\u001b[39m1\u001b[39m,\u001b[39m380\u001b[39m,\u001b[39m5\u001b[39m,\u001b[39m3\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kevin/OneDrive/Desktop/MASTERS/SYDE%20Project/model.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m: torch\u001b[39m.\u001b[39mones_like(torch\u001b[39m.\u001b[39mrand(\u001b[39m1\u001b[39m,\u001b[39m32\u001b[39m))         \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kevin/OneDrive/Desktop/MASTERS/SYDE%20Project/model.ipynb#X14sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m }\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kevin/OneDrive/Desktop/MASTERS/SYDE%20Project/model.ipynb#X14sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m model \u001b[39m=\u001b[39m Net()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/kevin/OneDrive/Desktop/MASTERS/SYDE%20Project/model.ipynb#X14sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(model(input_dict))\n",
      "File \u001b[1;32mc:\\Users\\kevin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kevin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\kevin\\OneDrive\\Desktop\\MASTERS\\SYDE Project\\model.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/kevin/OneDrive/Desktop/MASTERS/SYDE%20Project/model.ipynb#X14sZmlsZQ%3D%3D?line=142'>143</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_extractor(x, mask)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/kevin/OneDrive/Desktop/MASTERS/SYDE%20Project/model.ipynb#X14sZmlsZQ%3D%3D?line=143'>144</a>\u001b[0m x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m x1\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/kevin/OneDrive/Desktop/MASTERS/SYDE%20Project/model.ipynb#X14sZmlsZQ%3D%3D?line=144'>145</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(x, mask)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/kevin/OneDrive/Desktop/MASTERS/SYDE%20Project/model.ipynb#X14sZmlsZQ%3D%3D?line=145'>146</a>\u001b[0m decoder_labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mclone()        \n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/kevin/OneDrive/Desktop/MASTERS/SYDE%20Project/model.ipynb#X14sZmlsZQ%3D%3D?line=147'>148</a>\u001b[0m \u001b[39m#??\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/kevin/OneDrive/Desktop/MASTERS/SYDE%20Project/model.ipynb#X14sZmlsZQ%3D%3D?line=148'>149</a>\u001b[0m \u001b[39m# if self.training:\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/kevin/OneDrive/Desktop/MASTERS/SYDE%20Project/model.ipynb#X14sZmlsZQ%3D%3D?line=149'>150</a>\u001b[0m \u001b[39m#     m = torch.rand(labels.shape) < self.decoder_mask_aug\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/kevin/OneDrive/Desktop/MASTERS/SYDE%20Project/model.ipynb#X14sZmlsZQ%3D%3D?line=150'>151</a>\u001b[0m \u001b[39m#     decoder_labels[m] = 62\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kevin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kevin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kevin\\OneDrive\\Desktop\\MASTERS\\SYDE Project\\Squeezeformer.py:576\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(self, x, mask)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList()\n\u001b[0;32m    574\u001b[0m \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_layers):\n\u001b[0;32m    575\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks\u001b[39m.\u001b[39mappend(\n\u001b[1;32m--> 576\u001b[0m         SqueezeformerBlock(\n\u001b[0;32m    577\u001b[0m             encoder_dim\u001b[39m=\u001b[39mencoder_dim,\n\u001b[0;32m    578\u001b[0m             num_attention_heads\u001b[39m=\u001b[39mnum_attention_heads,\n\u001b[0;32m    579\u001b[0m             feed_forward_expansion_factor\u001b[39m=\u001b[39mfeed_forward_expansion_factor,\n\u001b[0;32m    580\u001b[0m             conv_expansion_factor\u001b[39m=\u001b[39mconv_expansion_factor,\n\u001b[0;32m    581\u001b[0m             feed_forward_dropout_p\u001b[39m=\u001b[39mfeed_forward_dropout_p,\n\u001b[0;32m    582\u001b[0m             attention_dropout_p\u001b[39m=\u001b[39mattention_dropout_p,\n\u001b[0;32m    583\u001b[0m             conv_dropout_p\u001b[39m=\u001b[39mconv_dropout_p,\n\u001b[0;32m    584\u001b[0m             conv_kernel_size\u001b[39m=\u001b[39mconv_kernel_size,\n\u001b[0;32m    585\u001b[0m         )\n\u001b[0;32m    586\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\kevin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1693\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[0;32m   1694\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1695\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SqueezeformerEncoder' object has no attribute 'blocks'"
     ]
    }
   ],
   "source": [
    "input_dict = {'lhand': torch.rand(1,380,21,3),\n",
    "            'rhand':torch.rand(1,380,21,3),\n",
    "            'lpose':torch.rand(1,380,5,3),\n",
    "            'rpose':torch.rand(1,380,5,3),\n",
    "            'token_ids':(torch.rand(1,32)*52).long(),\n",
    "            'input_mask': torch.ones_like(torch.rand(1,380)),\n",
    "            'attention_mask': torch.ones_like(torch.rand(1,32))         \n",
    "}\n",
    "model = Net()\n",
    "\n",
    "print(model(input_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, loss_fn, optimizer):\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    loss_history = []\n",
    "\n",
    "    model = model.to(DEVICE)\n",
    "    # set model to training mode\n",
    "    model.train()  \n",
    "\n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets.to(DEVICE))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # track some values to compute statistics\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(outputs, dim=-1)\n",
    "        all_predictions.extend(preds.detach().cpu().tolist())\n",
    "        all_targets.extend(targets.cpu().tolist())\n",
    "\n",
    "    acc = accuracy_score(all_targets, all_predictions)\n",
    "    final_loss = total_loss / len(train_loader)\n",
    "    \n",
    "    # print average loss and accuracy\n",
    "    print(f\"learning Rate = {optimizer.param_groups[0]['lr']}. average train loss = {final_loss:.2f}, average train accuracy = {acc * 100:.3f}%\")\n",
    "    return acc, final_loss\n",
    "\n",
    "def validation(model, val_loader, loss_fn,epoch):\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    model = model.to(DEVICE)\n",
    "    # set model to evaluation mode\n",
    "    model.eval()  \n",
    "    for i, (inputs, targets) in enumerate(val_loader):\n",
    "        with torch.no_grad():\n",
    "            model.to(DEVICE)\n",
    "            outputs = model(inputs.to(DEVICE))\n",
    "            loss = loss_fn(outputs, targets.to(DEVICE))\n",
    "\n",
    "            # Track some values to compute statistics\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=-1)\n",
    "            all_predictions.extend(preds.detach().cpu().tolist())\n",
    "            all_targets.extend(targets.cpu().tolist())\n",
    "\n",
    "    acc = accuracy_score(all_targets, all_predictions)\n",
    "    final_loss = total_loss / len(val_loader)\n",
    "    # Print average loss and accuracy\n",
    "    print(f\"Epoch {epoch + 1} done. average validation loss = {final_loss:.2f}, average validation accuracy = {acc * 100:.3f}%\")\n",
    "    return acc, final_loss\n",
    "\n",
    "def test(model, test_loader, loss_fn):\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    model = model.to(DEVICE)\n",
    "    # set model to evaluation mode\n",
    "    model.eval()\n",
    "    for i, (inputs, targets) in enumerate(test_loader):\n",
    "        with torch.no_grad():\n",
    "            model.to(DEVICE)\n",
    "            outputs = model(inputs.to(DEVICE))\n",
    "            loss = loss_fn(outputs, targets.to(DEVICE))\n",
    "\n",
    "            # Track some values to compute statistics\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=-1)\n",
    "            all_predictions.extend(preds.detach().cpu().tolist())\n",
    "            all_targets.extend(targets.cpu().tolist())\n",
    "\n",
    "    acc = accuracy_score(all_targets, all_predictions)\n",
    "    final_loss = total_loss / len(test_loader)\n",
    "    # Print average loss and accuracy\n",
    "    print(f'average test loss = {final_loss:.2f}, average test accuracy = {acc * 100:.3f}%')\n",
    "    return acc, final_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "                data_loaders: tuple = None,\n",
    "                learning_rate: float = 1e-4, \n",
    "                weight_decay: float = 1e-4, \n",
    "                features: dict = None\n",
    "               ):\n",
    "    \n",
    "    if not features:\n",
    "        features = FEATURES\n",
    "        \n",
    "    if not data_loaders:\n",
    "        train_loader, val_loader, test_loader = get_data_loaders(features, VAL_SPLIT, TEST_SPLIT, BATCH_SIZE)\n",
    "    else:\n",
    "        train_loader, val_loader, test_loader = data_loaders\n",
    "\n",
    "    model = None\n",
    "    optimizer = None\n",
    "    scheduler = None\n",
    "\n",
    "    max_epochs = 16\n",
    "    best_acc = -1\n",
    "    dip_count = 0\n",
    "    num_epochs = 0\n",
    "    for e in range(max_epochs):\n",
    "        print(f'EPOCH {e}:')\n",
    "        train_acc, train_loss = train(model, train_loader, loss_fn, optimizer)\n",
    "        scheduler.step()\n",
    "        \n",
    "         # early stopping based on train acc\n",
    "        if e%2 == 0:\n",
    "            val_acc, val_loss = validation(model, val_loader, loss_fn)\n",
    "            if val_acc >= best_acc:\n",
    "                best_acc = val_acc\n",
    "                dip_count = 0\n",
    "            else:\n",
    "                dip_count +=1\n",
    "\n",
    "        if dip_count >1:\n",
    "            break\n",
    "\n",
    "    val_acc, val_loss = validation(model, val_loader, loss_fn)\n",
    "    print(f'Final Accuracy: {val_acc}')\n",
    "    return val_acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
