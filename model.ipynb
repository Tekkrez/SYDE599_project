{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: timm in c:\\users\\kevin\\anaconda3\\lib\\site-packages (0.9.11)\n",
      "Requirement already satisfied: torch>=1.7 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from timm) (2.1.1+cu118)\n",
      "Requirement already satisfied: torchvision in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from timm) (0.16.1+cu118)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from timm) (6.0)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from timm) (0.15.1)\n",
      "Requirement already satisfied: safetensors in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from timm) (0.3.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from torch>=1.7->timm) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from torch>=1.7->timm) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from torch>=1.7->timm) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from torch>=1.7->timm) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from torch>=1.7->timm) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from torch>=1.7->timm) (2023.4.0)\n",
      "Requirement already satisfied: requests in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from huggingface-hub->timm) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from huggingface-hub->timm) (4.65.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from huggingface-hub->timm) (23.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from torchvision->timm) (1.24.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from torchvision->timm) (9.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub->timm) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.7->timm) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from requests->huggingface-hub->timm) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from requests->huggingface-hub->timm) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from requests->huggingface-hub->timm) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from requests->huggingface-hub->timm) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from sympy->torch>=1.7->timm) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import math\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from transformers.models.speech_to_text import Speech2TextConfig\n",
    "from transformers.models.speech_to_text.modeling_speech_to_text import shift_tokens_right, Speech2TextDecoder\n",
    "from Squeezeformer import SqueezeformerEncoder\n",
    "from timm.layers.norm_act import BatchNormAct2d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "FEATURES = {\n",
    "    'hand': {\n",
    "        'left': range(21),\n",
    "        'right': range(21)\n",
    "    },\n",
    "    'pose': {\n",
    "        'left': [13, 15, 17, 19, 21],\n",
    "        'right': [14, 16, 18, 20, 22]\n",
    "    },\n",
    "    #'head': range(468)\n",
    "}\n",
    "\n",
    "FRAME_LEN = 128\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "VAL_SPLIT = 0.8\n",
    "TEST_SPLIT = 0.2\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FingerspellingDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 features: dict = FEATURES,\n",
    "                 train: str = True,\n",
    "                 transform = None\n",
    "                ):\n",
    "        HAND_COLS = []\n",
    "        if 'hand' in features:\n",
    "            HAND_COLS = [\n",
    "                f'{d}_{o}_hand_{i}' \n",
    "                for o in ['right', 'left'] \n",
    "                for i in features['hand'][o] \n",
    "                for d in ['x', 'y', 'z']\n",
    "            ]\n",
    "        POSE_COLS = []\n",
    "        if 'pose' in features:\n",
    "            POSE_COLS = [\n",
    "                f'{d}_pose_{i}' \n",
    "                for o in ['right', 'left'] \n",
    "                for i in features['pose'][o]\n",
    "                for d in ['x', 'y', 'z']\n",
    "            ]\n",
    "        HEAD_COLS = []\n",
    "        if 'head' in features:\n",
    "            HEAD_COLS = [\n",
    "                f'{d}_head_{i}' \n",
    "                for i in features['head'] \n",
    "                for d in ['x', 'y', 'z']\n",
    "            ]\n",
    " \n",
    "        if train:\n",
    "            # dataset_path = '/kaggle/input/asl-fingerspelling/train_landmarks'\n",
    "            # dataset_file = '/kaggle/input/asl-fingerspelling/train.csv'\n",
    "            dataset_path = 'C:/Users/kevin/OneDrive/Desktop/MASTERS/SYDE Project/asl-fingerspelling/train_landmarks'\n",
    "            dataset_file = 'C:/Users/kevin/OneDrive/Desktop/MASTERS/SYDE Project/asl-fingerspelling/train.csv'\n",
    "        else:\n",
    "            # dataset_path = '/kaggle/input/asl-fingerspelling/supplemental_landmarks'\n",
    "            # dataset_file = '/kaggle/input/asl-fingerspelling/supplemental_metadata.csv'\n",
    "            dataset_path = 'C:/Users/kevin/OneDrive/Desktop/MASTERS/SYDE Project/asl-fingerspelling/supplemental_landmarks'\n",
    "            dataset_file = 'C:/Users/kevin/OneDrive/Desktop/MASTERS/SYDE Project/asl-fingerspelling/supplemental_metadata.csv'\n",
    "        self.dataset_path = dataset_path\n",
    "        self.dataset_df = pd.read_csv(dataset_file)\n",
    "        self.feature_columns = HAND_COLS + POSE_COLS + HEAD_COLS\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.dataset_df)\n",
    "    def __getitem__(self, index):\n",
    "        # convert to list of indices (if index is a tensor)\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "        # locate sample in dataset dataframe\n",
    "        sequence_id, file_id, phrase = self.dataset_df.iloc[index][['sequence_id', 'file_id', 'phrase']]\n",
    "        # filter dataset and fetch entries for the relevant file_id\n",
    "        file_df = self.dataset_df.loc[self.dataset_df[\"file_id\"] == file_id]\n",
    "        # fetch the data from the .parquet file\n",
    "        # filter out the non used columns\n",
    "        parquet_df = pq.read_table(\n",
    "                        f\"{self.dataset_path}/{str(file_id)}.parquet\",\n",
    "                        columns=['sequence_id'] + self.feature_columns\n",
    "                     ).to_pandas()\n",
    "        # convert parquet data to numpy\n",
    "        parquet_np = parquet_df.to_numpy()\n",
    "        # filter the parquet data by the sequence_id of the sample\n",
    "        frames = parquet_np[parquet_df.index == sequence_id]\n",
    " \n",
    "        sample = {\n",
    "            'data': frames, # numpy.ndarray\n",
    "            'phrase': phrase, # string\n",
    "        }\n",
    "        # apply transformation(s)\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    " \n",
    "    def __call__(self, sample):\n",
    "        frames, phrase = sample['data'], sample['phrase']\n",
    "        return {\n",
    "            'data': torch.from_numpy(frames), \n",
    "            'phrase': phrase\n",
    "        }\n",
    "\n",
    "def get_data_loaders(features: dict = FEATURES, val_split: float = 0.8, test_split: float = 0.2, batch_size: int = 16):\n",
    "    # load datasets\n",
    "    transform = transforms.Compose([ToTensor()])\n",
    "    train_data = FingerspellingDataset(features=features, train=True, transform=transform) # need to add transforms\n",
    "    test_val_data = FingerspellingDataset(features=features, train=False, transform=transform) # need to add transforms\n",
    "    dataset_size = len(test_val_data)\n",
    "    test_data, val_data = torch.utils.data.random_split(test_val_data, [math.ceil(test_split * dataset_size), math.floor(val_split * dataset_size)])\n",
    " \n",
    "    # setup data loaders\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=1000)\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[ 2.2331421e-01,  7.6893485e-01, -5.7574545e-07, ...,\n",
       "          1.0169585e+00,  1.2544518e+00, -1.4506084e+00],\n",
       "        [ 2.2377665e-01,  7.6065981e-01, -5.3920235e-07, ...,\n",
       "          1.0140365e+00,  1.2621617e+00, -1.4357543e+00],\n",
       "        [ 2.2664991e-01,  7.5464553e-01, -6.1104475e-07, ...,\n",
       "          1.0186688e+00,  1.2638935e+00, -1.4096643e+00],\n",
       "        ...,\n",
       "        [ 2.1587229e-01,  7.5916189e-01, -7.2643348e-07, ...,\n",
       "          1.0948149e+00,  1.2413069e+00, -1.1267381e+00],\n",
       "        [           nan,            nan,            nan, ...,\n",
       "          1.0952215e+00,  1.2484142e+00, -1.1213843e+00],\n",
       "        [ 2.1843103e-01,  7.6109159e-01, -7.0708330e-07, ...,\n",
       "          1.1058801e+00,  1.2580189e+00, -1.2688636e+00]], dtype=float32),\n",
       " 'phrase': 'scales/kuhaylah'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = FingerspellingDataset() # need to add transforms\n",
    "train_data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_landmarks,\n",
    "                 out_dim):\n",
    "        super().__init__()   \n",
    "\n",
    "        self.in_channels = in_channels = (32//2) * n_landmarks\n",
    "        self.stem_linear = nn.Linear(in_channels,out_dim,bias=False)\n",
    "        self.stem_bn = nn.BatchNorm1d(out_dim, momentum=0.95)\n",
    "        self.conv_stem = nn.Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 2), padding=(1, 1), bias=False)\n",
    "        self.bn_conv = BatchNormAct2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True,act_layer = nn.SiLU,drop_layer=None)\n",
    "        \n",
    "    def forward(self, data, mask):\n",
    "\n",
    "        xc = data.permute(0,3,1,2)\n",
    "        xc = self.conv_stem(xc)\n",
    "        xc = self.bn_conv(xc)\n",
    "        xc = xc.permute(0,2,3,1)\n",
    "        xc = xc.reshape(*data.shape[:2], -1)\n",
    "        \n",
    "        m = mask.to(torch.bool)  \n",
    "        x = self.stem_linear(xc)\n",
    "        \n",
    "        # Batchnorm without pads\n",
    "        bs,slen,nfeat = x.shape\n",
    "        x = x.view(-1, nfeat)\n",
    "        x_bn = x[mask.view(-1)==1].unsqueeze(0)\n",
    "        x_bn = self.stem_bn(x_bn.permute(0,2,1)).permute(0,2,1)\n",
    "        x[mask.view(-1)==1] = x_bn[0]\n",
    "        x = x.view(bs,slen,nfeat)\n",
    "        # Padding mask\n",
    "        x = x.masked_fill(~mask.bool().unsqueeze(-1), 0.0)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, decoder_config):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.config = decoder_config\n",
    "        self.decoder = Speech2TextDecoder(decoder_config) \n",
    "        self.lm_head = nn.Linear(decoder_config.d_model, decoder_config.vocab_size, bias=False)\n",
    "        \n",
    "        self.decoder_start_token_id = decoder_config.decoder_start_token_id\n",
    "        self.decoder_pad_token_id = decoder_config.pad_token_id #used for early stopping\n",
    "        self.decoder_end_token_id= decoder_config.eos_token_id\n",
    "        \n",
    "    def forward(self,x, labels=None, attention_mask = None, encoder_attention_mask = None):\n",
    "        \n",
    "        if labels is not None:\n",
    "            decoder_input_ids = shift_tokens_right(labels, self.config.pad_token_id, self.config.decoder_start_token_id)\n",
    "            \n",
    "        decoder_outputs = self.decoder(input_ids=decoder_input_ids,\n",
    "                                       encoder_hidden_states=x, \n",
    "                                       attention_mask = attention_mask,\n",
    "                                       encoder_attention_mask = encoder_attention_mask)\n",
    "        lm_logits = self.lm_head(decoder_outputs.last_hidden_state)\n",
    "        return lm_logits\n",
    "            \n",
    "    def generate(self, x, max_new_tokens=33, encoder_attention_mask=None):\n",
    "\n",
    "        decoder_input_ids = torch.ones((x.shape[0], 1), device=x.device, dtype=torch.long).fill_(self.decoder_start_token_id)\n",
    "        for i in range(max_new_tokens-1):  \n",
    "            decoder_outputs = self.decoder(input_ids=decoder_input_ids,encoder_hidden_states=x, encoder_attention_mask=encoder_attention_mask)\n",
    "            logits = self.lm_head(decoder_outputs.last_hidden_state)\n",
    "            decoder_input_ids = torch.cat([decoder_input_ids,logits.argmax(2)[:,-1:]],dim=1)\n",
    "\n",
    "            if torch.all((decoder_input_ids==self.decoder_end_token_id).sum(-1) > 0):\n",
    "                break\n",
    "                \n",
    "        return decoder_input_ids\n",
    "    \n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "\n",
    "        dim=208\n",
    "        n_handlandmarks = 21\n",
    "        n_poselandmarks = 5\n",
    "        n_landmarks = 2*n_handlandmarks+2*n_poselandmarks\n",
    "\n",
    "        d_cfg = Speech2TextConfig.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
    "        d_cfg.encoder_layers = 0\n",
    "        d_cfg.decoder_layers = 2\n",
    "        d_cfg.d_model = dim\n",
    "        d_cfg.max_target_positions = 1024 #?\n",
    "        d_cfg.num_hidden_layers = 1\n",
    "        d_cfg.vocab_size = 63\n",
    "        d_cfg.bos_token_id = 59\n",
    "        d_cfg.eos_token_id = 60\n",
    "        d_cfg.decoder_start_token_id = 59\n",
    "        d_cfg.pad_token_id = 61\n",
    "        d_cfg.num_conv_layers = 0\n",
    "        d_cfg.conv_kernel_sizes = []\n",
    "        d_cfg.max_length = dim\n",
    "        d_cfg.input_feat_per_channel = dim\n",
    "        d_cfg.num_beams = 1\n",
    "        d_cfg.attention_dropout = 0.2\n",
    "        d_cfg.decoder_ffn_dim = 512\n",
    "        d_cfg.init_std = 0.02\n",
    "        \n",
    "        self.feature_extractor = FeatureExtractor(n_landmarks=n_landmarks,out_dim=dim)\n",
    "        self.feature_extractor_lhand = FeatureExtractor(n_handlandmarks,out_dim=dim//4)\n",
    "        self.feature_extractor_rhand = FeatureExtractor(n_handlandmarks,out_dim=dim//4)\n",
    "        self.feature_extractor_lpose = FeatureExtractor(n_poselandmarks,out_dim=dim//4)\n",
    "        self.feature_extractor_rpose = FeatureExtractor(n_poselandmarks,out_dim=dim//4)\n",
    "        \n",
    "        self.encoder = SqueezeformerEncoder(\n",
    "                      input_dim=dim,\n",
    "                      encoder_dim=dim,\n",
    "                      num_layers=5,\n",
    "                      num_attention_heads= 4,\n",
    "                      feed_forward_expansion_factor=1,\n",
    "                      conv_expansion_factor= 2,\n",
    "                      input_dropout_p=0.1,\n",
    "                      feed_forward_dropout_p= 0.1,\n",
    "                      attention_dropout_p= 0.1,\n",
    "                      conv_dropout_p= 0.1,\n",
    "                      conv_kernel_size= 51,)\n",
    "        self.decoder = Decoder(d_cfg)\n",
    "        self.loss_fn = nn.CrossEntropyLoss() #done\n",
    "        print('n_params:',count_parameters(self))\n",
    "\n",
    "    def forward(self, batch):\n",
    "        #Concat,'rhand','lpose','rpose'\n",
    "        x = torch.cat(batch['lhand'],batch['rhand'],batch['lpose'],batch['rpose'],2)\n",
    "        labels = batch['token_ids']\n",
    "        mask = batch['input_mask'].long()\n",
    "        label_mask = batch['attention_mask']    \n",
    "\n",
    "        #maybe normalize\n",
    "\n",
    "        x_lhand = self.feature_extractor_lhand(batch['lhand'].clone(), mask)\n",
    "        x_rhand = self.feature_extractor_rhand(batch['rhand'].clone(), mask)\n",
    "        x_lpose = self.feature_extractor_lpose(batch['lpose'].clone(), mask)\n",
    "        x_rpose = self.feature_extractor_rpose(batch['rpose'].clone(), mask)\n",
    "        \n",
    "        x1 = torch.cat([x_lhand,x_rhand,x_lpose,x_rpose],dim=-1)\n",
    "        x = self.feature_extractor(x, mask)\n",
    "        x = x + x1\n",
    "        x = self.encoder(x, mask)\n",
    "        decoder_labels = labels.clone()        \n",
    "        \n",
    "        #??\n",
    "        if self.training:\n",
    "            m = torch.rand(labels.shape) < self.decoder_mask_aug\n",
    "            decoder_labels[m] = 62\n",
    "\n",
    "        logits = self.decoder(x,\n",
    "                            labels=decoder_labels, \n",
    "                            encoder_attention_mask=mask.long()\n",
    "                            )\n",
    "        \n",
    "        loss = self.loss_fn(logits.view(-1, self.decoder.config.vocab_size), labels.view(-1))\n",
    "\n",
    "        output = {'loss':loss}\n",
    "\n",
    "        if not self.training:\n",
    "            generated_ids_padded = torch.ones((x.shape[0],self.max_phrase), dtype=torch.long, device=x.device) * 59\n",
    "            \n",
    "            if self.val_mode == 'padded':\n",
    "                generated_ids = self.decoder.generate(x,max_new_tokens=self.max_phrase + 1, encoder_attention_mask=mask.long())\n",
    "                    \n",
    "            elif self.val_mode == 'cutted':\n",
    "                generated_ids = torch.ones((x.shape[0],self.max_phrase+1), dtype=torch.long, device=x.device) * 59\n",
    "                mask_lens = mask.sum(1)\n",
    "                for lidx in mask_lens.unique():\n",
    "                    liddx = lidx == mask_lens\n",
    "                    preds = self.decoder.generate(x[liddx, :lidx],max_new_tokens=self.max_phrase + 1)\n",
    "                    generated_ids[liddx, :preds.shape[1]] = preds\n",
    "                    \n",
    "            cutoffs = (generated_ids==self.decoder.decoder_end_token_id).float().argmax(1).clamp(0,self.max_phrase)\n",
    "            for i, c in enumerate(cutoffs):\n",
    "                generated_ids_padded[i,:c] = generated_ids[i,:c]\n",
    "            output['generated_ids'] = generated_ids_padded\n",
    "            output['seq_len'] = batch['seq_len']    \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, loss_fn, optimizer):\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    loss_history = []\n",
    "\n",
    "    model = model.to(DEVICE)\n",
    "    # set model to training mode\n",
    "    model.train()  \n",
    "\n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets.to(DEVICE))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # track some values to compute statistics\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(outputs, dim=-1)\n",
    "        all_predictions.extend(preds.detach().cpu().tolist())\n",
    "        all_targets.extend(targets.cpu().tolist())\n",
    "\n",
    "    acc = accuracy_score(all_targets, all_predictions)\n",
    "    final_loss = total_loss / len(train_loader)\n",
    "    \n",
    "    # print average loss and accuracy\n",
    "    print(f\"learning Rate = {optimizer.param_groups[0]['lr']}. average train loss = {final_loss:.2f}, average train accuracy = {acc * 100:.3f}%\")\n",
    "    return acc, final_loss\n",
    "\n",
    "def validation(model, val_loader, loss_fn,epoch):\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    model = model.to(DEVICE)\n",
    "    # set model to evaluation mode\n",
    "    model.eval()  \n",
    "    for i, (inputs, targets) in enumerate(val_loader):\n",
    "        with torch.no_grad():\n",
    "            model.to(DEVICE)\n",
    "            outputs = model(inputs.to(DEVICE))\n",
    "            loss = loss_fn(outputs, targets.to(DEVICE))\n",
    "\n",
    "            # Track some values to compute statistics\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=-1)\n",
    "            all_predictions.extend(preds.detach().cpu().tolist())\n",
    "            all_targets.extend(targets.cpu().tolist())\n",
    "\n",
    "    acc = accuracy_score(all_targets, all_predictions)\n",
    "    final_loss = total_loss / len(val_loader)\n",
    "    # Print average loss and accuracy\n",
    "    print(f\"Epoch {epoch + 1} done. average validation loss = {final_loss:.2f}, average validation accuracy = {acc * 100:.3f}%\")\n",
    "    return acc, final_loss\n",
    "\n",
    "def test(model, test_loader, loss_fn):\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    model = model.to(DEVICE)\n",
    "    # set model to evaluation mode\n",
    "    model.eval()\n",
    "    for i, (inputs, targets) in enumerate(test_loader):\n",
    "        with torch.no_grad():\n",
    "            model.to(DEVICE)\n",
    "            outputs = model(inputs.to(DEVICE))\n",
    "            loss = loss_fn(outputs, targets.to(DEVICE))\n",
    "\n",
    "            # Track some values to compute statistics\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=-1)\n",
    "            all_predictions.extend(preds.detach().cpu().tolist())\n",
    "            all_targets.extend(targets.cpu().tolist())\n",
    "\n",
    "    acc = accuracy_score(all_targets, all_predictions)\n",
    "    final_loss = total_loss / len(test_loader)\n",
    "    # Print average loss and accuracy\n",
    "    print(f'average test loss = {final_loss:.2f}, average test accuracy = {acc * 100:.3f}%')\n",
    "    return acc, final_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "                data_loaders: tuple = None,\n",
    "                learning_rate: float = 1e-4, \n",
    "                weight_decay: float = 1e-4, \n",
    "                features: dict = None\n",
    "               ):\n",
    "    \n",
    "    if not features:\n",
    "        features = FEATURES\n",
    "        \n",
    "    if not data_loaders:\n",
    "        train_loader, val_loader, test_loader = get_data_loaders(features, VAL_SPLIT, TEST_SPLIT, BATCH_SIZE)\n",
    "    else:\n",
    "        train_loader, val_loader, test_loader = data_loaders\n",
    "\n",
    "    model = None\n",
    "    optimizer = None\n",
    "    scheduler = None\n",
    "\n",
    "    max_epochs = 16\n",
    "    best_acc = -1\n",
    "    dip_count = 0\n",
    "    num_epochs = 0\n",
    "    for e in range(max_epochs):\n",
    "        print(f'EPOCH {e}:')\n",
    "        train_acc, train_loss = train(model, train_loader, loss_fn, optimizer)\n",
    "        scheduler.step()\n",
    "        \n",
    "         # early stopping based on train acc\n",
    "        if e%2 == 0:\n",
    "            val_acc, val_loss = validation(model, val_loader, loss_fn)\n",
    "            if val_acc >= best_acc:\n",
    "                best_acc = val_acc\n",
    "                dip_count = 0\n",
    "            else:\n",
    "                dip_count +=1\n",
    "\n",
    "        if dip_count >1:\n",
    "            break\n",
    "\n",
    "    val_acc, val_loss = validation(model, val_loader, loss_fn)\n",
    "    print(f'Final Accuracy: {val_acc}')\n",
    "    return val_acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
